#!/bin/sh
#
# Description: The control program for build a ha system.
#              Main program: (1) prepare. (2) install (3) failover (4) recover
# Authors:      Qin Guanri
# Copyright:    2016 403709339@qq.com
###############################################################################

WORK_DIR=$(cd `dirname $0`; pwd)
INSTALL_DIR=$(cd $WORK_DIR; cd ../; pwd)
PROC_NAME="skylar_ha"
PROC_FILE="$PROC_NAME.tar.gz"
RECOVERY_LOG="/var/log/skyha_recovery.log"
losetup_dev_default=/dev/loop9

usage() {
    cat <<END
skyha build a High Availabe system for postgresql/redis/beanstalkd.

usage: skyha <command> [<args>]

The most commonly used skyha commands are:
    reset       Clean configs.
    failover    Config failover for ha system. $0 failover <pg|redis>. e.g: $0 failover pg or $0 failover redis.
    help        Show usage.
    install     Install a ha system. $0 install master_ip=1.2.3.4 slave_ip=1.2.3.5 vip_master=1.2.3.6 \
                data_dir=/data master_hostname=db1 slave_hostname=db2 drbd_size=1000 losetup_dev=/dev/loop9
    config      Set master_ip, slave_ip and so on. $0 config master_ip=1.2.3.4 slave_ip=1.2.3.5 vip_master=1.2.3.6 \
                data_dir=/data master_hostname=db1 slave_hostname=db2 drbd_size=1000 losetup_dev=/dev/loop9
    recover     WHen master or slave crash, we need to 'recover' it. $0 recover.
    enable      Enable some function. e.g: $0 enable auto_recovery.
    disable     Disable some function. e.g: $0 disable auto_recovery.
    show        Show HA status.
    switch-master-slave Switch the role of master and slave. This command can only do on master node.
END
}


# do prepare before install.  prepare for environment, check args, check files and so on.
prepare() {
    if [ ! -f "$WORK_DIR/conf/ha.conf" ]; then
        echo "ERROR. can not find file: $WORK_DIR/conf/ha.conf"
        return 1
    fi

    if ! systemctl status docker.service; then
        systemctl start docker.service
        if [ $? -ne 0 ]; then
            echo "ERROR. start docker.service failed."
            return 1
        fi
    fi

    source $WORK_DIR/conf/ha.conf

    mkdir -p $INSTALL_DIR

    kill -9 $(ps -ef | grep 'yum' | awk {'print $2'})

    chmod +x $WORK_DIR/manager/* $WORK_DIR/resource/* $WORK_DIR/include/* $WORK_DIR/skyha

    $WORK_DIR/include/common "create_repo"

    for file in $WORK_DIR/manager/* $WORK_DIR/resource/*
    do
        if [ -f $file ]; then
            $file "prepare"
            if [ $? -ne 0 ]; then
                echo "ERROR. Execute $file prepare failed."
                return 1
            fi
        fi
    done

    systemctl restart iptables.service
    return 0
}


# invoke every manager's and resource's install functions. install manager at first, then install resource
install() {
    kill -9 $(ps -ef | grep 'yum' | awk {'print $2'})
    for file in $WORK_DIR/manager/* $WORK_DIR/resource/*
    do
        if [ -f $file ]; then
            $file "install"
            if [ $? -ne 0 ]; then
                echo "ERROR. execute $file install failed."
                return 1
            fi
        fi
    done
    return 0
}


# failover $mode.   $mode can be STANDALONE or MS.  config failover for each resource. when it finished, resource should be reach on ha status.
failover() {
    mode=$1
    if [ "$mode" != "STANDALONE" ]; then
        echo "set drbd master ..."
        $WORK_DIR/resource/drbd set_master
        if [ $? -ne 0 ]; then
            echo "drbd set master failed."
            return 1
        fi
    fi

    pacemaker_proc=`ps -ef | grep pacemaker| grep -v grep | wc -l`
    if [ $pacemaker_proc -gt 0 ]; then
        kill -9 $(ps -ef | grep pacemaker| grep -v grep | awk {'print $2'})
    fi
    sleep 1

    systemctl stop pacemaker.service
    rm -rf /var/lib/pacemaker/cib/cib*

    if [ "$mode" != "STANDALONE" ]; then
        rm -f /etc/corosync/corosync.conf
    fi

    systemctl restart pcsd.service
    systemctl enable pcsd.service
    sleep 5

    pcs cluster auth -u hacluster -p hacluster $master_hostname $slave_hostname
    pcs cluster setup --last_man_standing=1 --name skycluster $master_hostname $slave_hostname --force
    pcs cluster start --all

    sleep 2
    master_corosync_run=`pcs status corosync | grep $master_hostname | grep -v grep | wc -l`
    slave_corosync_run=`pcs status corosync | grep $slave_hostname | grep -v grep | wc -l`

    if [ "$mode" == "STANDALONE" ]; then
        echo "failover mode is $mode"
    elif [ "$master_corosync_run" == "0" ] || [ "$slave_corosync_run" == "0" ]; then
        echo "ERROR. master's corosync or slave's corosync run error."
        pcs status corosync
        return 1
    fi

    systemctl start pacemaker.service
    try_count=1
    MAX_TRY=60
    while [ $try_count -lt $MAX_TRY ]
    do
        if systemctl status pacemaker.service; then
            break
        fi
        echo "pacemaker is starting ..."
        sleep 1
        let "try_count++"
    done
    if ! systemctl status pacemaker.service; then
        echo "ERROR. pacemaker start failed."
        return 1
    fi

    cidr_netmask=`ip addr | grep $master_ip | grep '/' |awk {'print $2'} | awk -F '/' {'print $2'}`
    MIN=0
    MAX=32
    if [ "$cidr_netmask" -lt "$MIN" ] || [ "$cidr_netmask" -gt "$MAX" ]; then
        echo "ERROR. cidr_netmask error. use default"
        cidr_netmask="24"
    fi
 
    cd $WORK_DIR
    rm -f resource_cfg
    pcs cluster cib resource_cfg

    # 在pacemaker级别忽略quorum
    pcs -f resource_cfg property set no-quorum-policy="ignore"

    # 禁用STONITH
    pcs -f resource_cfg property set stonith-enabled="false"

    # 设置资源粘性，防止节点在故障恢复后发生迁移
    pcs -f resource_cfg resource defaults resource-stickiness="INFINITY"

    # 设置多少次失败后迁移
    pcs -f resource_cfg resource defaults migration-threshold="3" 
    pcs -f resource_cfg resource defaults failure-timeout="10s"

    # 设置master节点虚ip
    pcs -f resource_cfg resource create vip-master IPaddr2 ip="$vip_master" cidr_netmask="$cidr_netmask"\
        op start   timeout="60s" interval="0s"  on-fail="restart"\
        op monitor timeout="60s" interval="5s"  on-fail="restart"\
        op stop    timeout="60s" interval="0s"  on-fail="block"
    

    for file in $WORK_DIR/resource/*
    do
        if [ -f $file ]; then
            $file "failover"
            if [ $? -ne 0 ]; then
                echo "ERROR. execute $file failover failed."
                return 1
            fi
        fi
    done

    # HA组件分组:
    pcs -f resource_cfg resource group add master-group skyfs vip-master postgres redis bstkd nfs-daemon nfs-root

    ## HA组件运行位置约束： [vip+drbd-cluster-master+skyfs+pg-cluster-master+bstkd+redis+nfs]  都运行在一台机器上
    pcs -f resource_cfg constraint colocation add master-group with drbd-cluster INFINITY with-rsc-role=Master

    ## HA组件启动顺序约束：
    pcs -f resource_cfg constraint order promote drbd-cluster then start master-group score=INFINITY

    ## HA 首次启动位置约束
    pcs -f resource_cfg constraint location drbd-cluster prefers $master_hostname=10

    pcs cluster cib-push resource_cfg
    rm -f resource_cfg

    sleep 2
    pcs cluster unstandby --all

    if [ "$mode" == "STANDALONE" ]; then
        $WORK_DIR/resource/pg check_master_status
        if [ $? -ne 0 ]; then
            echo "ERROR. PG master status is error."
            return 1
        fi
        install_finished
        return 0
    fi

    check_ha_status
    if [ $? -ne 0 ]; then
        echo "ERROR. config failed."
        return 1  
    fi

    install_finished
}


# get args from input, and output args into ha.conf
args_parse() {
    if [ ! -f $WORK_DIR/conf/ha.conf ]; then
        arguments="$@"
        echo "$arguments" | xargs -n 1 > $WORK_DIR/conf/ha.conf
    fi

    source $WORK_DIR/conf/ha.conf

    my_ip="$master_ip"
    ip addr | grep "$slave_ip" >> /dev/null
    if [ $? -eq 0 ]; then
        my_ip="$slave_ip"
    fi

    if ! check_args_valid; then
        echo "ERROR. arguments invalid"
        usage
        exit 1
    fi
}


# invoke manager's and resource's clean method. clean some configs.
reset() {
    docker stop redis
    docker stop pg
    docker stop beanstalkd
    docker rm redis
    docker rm pg
    docker rm beanstalkd

    for file in $WORK_DIR/manager/* $WORK_DIR/resource/*
    do
        if [ -f $file ]; then
            $file "clean"
            if [ $? -ne 0 ]; then
                echo "ERROR. execute $file clean failed."
                return 1
            fi
        fi
    done

    echo "remove images ..."
    docker rmi skylar_redis
    docker rmi skylar_pg
    docker rmi skylar_beanstalkd
}


# invoke the resouce's recover method. recover resouce back to ha status.
recover() {
    if check_status; then
        echo "OK. HA status is correct."
        return 0
    fi

    date
    try=0
    while [ $try -le 5 ]
    do
        losetup -a | grep 'drbd.img'
        if [ $? -ne 0 ]; then
            losetup $losetup_dev $data_dir/drbd.img
        else
            break
        fi

        sleep 1
        let "try++"
    done

    systemctl start docker.service

    if ! master_is_work; then
        recover_master
        check_master_status
    elif ! slave_is_work; then
        ip addr | grep $vip_master
        if [ $? -ne 0 ]; then
            recover_slave $1
            check_ha_status
        fi
    fi

    if [ $? -ne 0 ]; then
        echo "ERROR. Recover failed. `date`"
        return 1
    fi

    echo "Done. Recover finished at time: `date`"
}

check_ha_status() {
    TRY=1 
    while [ $TRY -lt 60 ]
    do
        if check_status; then
            echo "OK. HA status is correct."
            return 0
        fi
        
        systemctl status pacemaker.service
        if [ $? -ne 0 ]; then
            systemctl start pacemaker.service
            sleep 2
        fi

        pcs cluster unstandby --all

        sleep 5
        let "TRY++"
    done
    echo "ERROR. HA status is incorrect."
    return 1
}

check_status() {
    for file in $WORK_DIR/resource/*
    do
        if [ -f $file ]; then
            $file "status"
            if [ $? -ne 0 ]; then
                echo "ERROR. HA status is incorrect. Execute $file status failed."
                return 1
            fi
        fi
    done
}

check_master_status() {
    echo "check master status ..."
    try=1

    while [ $try -lt 60 ]
    do
        sleep 1
        let "try++"
        pcs cluster unstandby $HOSTNAME

        crm_mon -Afr -1 | grep Masters
        if [ $? -ne 0 ]; then
            continue
        fi

        crm_mon -Afr -1 | grep skyfs | grep Started
        if [ $? -ne 0 ]; then
            continue
        fi

        crm_mon -Afr -1 | grep postgres | grep Started
        if [ $? -ne 0 ]; then
            continue
        fi

        cat /proc/drbd | grep Primary
        if [ $? -ne 0 ]; then
            continue
        fi
        
        echo "OK. Master status is correct."
        return 0
    done
    echo "ERROR. Master status is incorrect."
    return 1
}

master_is_work() {
    ping -c 1 "$vip_master"
}

slave_is_work() {
    check_status
}

recover_master() {
    echo "recover master, please wait minutes ..."
    pacemaker_proc=`ps -ef | grep pacemaker| grep -v grep | wc -l`
    if [ $pacemaker_proc -gt 0 ]; then
        kill -9 $(ps -ef | grep pacemaker| grep -v grep | awk {'print $2'})
    fi

    drbdadm up skydata
    drbdadm primary all
    drbdadm disconnect all
    drbdadm connect all

    systemctl start pacemaker.service
    sleep 1
    pcs cluster unstandby $HOSTNAME
}

recover_slave() {
    echo "recover slave ..."
    systemctl start docker.service
    systemctl start pacemaker.service
    pcs cluster unstandby --all
    sleep 5

    for file in $WORK_DIR/resource/*
    do
        if [ -f $file ]; then
            $file "recover"
            if [ $? -ne 0 ]; then
                echo "ERROR. Execute $file recover failed."
                return 1
            fi
        fi
    done
}


# check args and install just for one host.
run() {
    echo "(1) ******* Prepare environment ..."
    if ! prepare; then
        echo "ERROR. Prepare environment failed."
        exit 1
    fi

    echo "(2) ******* Install ..."
    if ! install; then
        echo "ERROR. Install skylar high available failed."
        exit 1
    fi

    enable "auto_recovery"
    echo "OK. Install succeed."
}

setup() {
    arguments=($@)
    unset arguments[0]
    args_parse ${arguments[@]} 
    run
}

install_finished() {
    echo '
                                 
            Skylar  High  Available
                Powered by Gary

              ___.-~"~-._   __....__
            .`    `    \ ~"~        ``-.
           /` _      )  `\              `\
          /`  a)    /     |               `\
         :`        /      |                 \
    <`-._|`  .-.  (      /   .            `;\\
     `-. `--`_.`-.;\___/`   .      .       | \\
  _     /:--`     |        /     /        .`  \\
 ("\   /`/        |       `     `         /    :`;
 `\`\_/`/         .\     /`~`--.:        /     ``
   `._.`          /`\    |      `\      /(
                 /  /\   |        `Y   /  \
                J  /  Y  |         |  /`\  \
               /  |   |  |         |  |  |  |
              "---"  /___|        /___|  /__|
                     `"""         `"""  `"""

    Congratulations! Installation completed! '
}

check_args_valid()
{
    $WORK_DIR/include/common check_args_valid
}


# install all resource and manager's for all hosts. Then config failover for all hosts.
install_all() {
    arguments=($@)
    unset arguments[0]
    args=${arguments[@]}
    args_parse ${arguments[@]}
    find $INSTALL_DIR -name "$PROC_FILE"
    if [ $? -ne 0 ]; then
        echo "ERROR. Cannot find file: $INSTALL_DIR/$PROC_FILE"
        return 1
    fi

    hostname -I | grep "$master_ip"
    if [ $? -ne 0 ]; then
        echo "ERROR. This host is not $master_ip, please kill this process then install at $master_ip."
        sleep 120
    fi

    clear
    echo "******* 1. Copy $PROC_FILE to root@$slave_ip:$INSTALL_DIR"
    ssh root@$slave_ip "ls -l $INSTALL_DIR/$PROC_NAME/skyha"
    if [ $? -ne 0 ]; then
        echo "copy file to root@$slave_ip, please wait seconds"
        ssh root@$slave_ip "mkdir -p $INSTALL_DIR"
        scp $INSTALL_DIR/$PROC_FILE root@$slave_ip:$INSTALL_DIR
        ssh root@$slave_ip "cd $INSTALL_DIR; tar xzvf $PROC_FILE; chmod +x $INSTALL_DIR/$PROC_NAME/skyha"
        if [ $? -ne 0 ]; then
            echo "ERROR. scp from $master_ip to $slave_ip failed."
            return 1
        fi
    fi
    scp $INSTALL_DIR/$PROC_NAME/conf/ha.conf root@$slave_ip:$INSTALL_DIR/$PROC_NAME/conf

    echo "******* 2. Install ..."
    run
    if [ $? -ne 0 ]; then
        echo "ERROR. Install at master failed."
        return 1
    fi
    
    ssh root@$slave_ip "$INSTALL_DIR/$PROC_NAME/skyha setup ${args[@]}"
    if [ $? -ne 0 ]; then
        echo "ERROR. Install at slave failed."
        return 1
    fi

    echo "******* 3. Config failover ..."
    failover
    if [ $? -ne 0 ]; then
        echo "ERROR. Config failover failed."
        return 1
    fi
}

config() {
    rm -f $WORK_DIR/conf/ha.conf
    arguments=($@)
    unset arguments[0]
    args_parse ${arguments[@]}
    echo "config succeed."
}

enable() {
    arg=$1
    if [ "$arg" == "auto_recovery" ]; then
        cat /etc/rc.d/rc.local | grep 'skyha boot'
        if [ $? -eq 0 ]; then
            return 0
        fi
        echo "$WORK_DIR/skyha boot >> $RECOVERY_LOG &" >> /etc/rc.d/rc.local
        chmod +x /etc/rc.local /etc/rc.d/rc.local
    fi
}

disable() {
    arg=$1
    if [ "$arg" == "auto_recovery" ]; then
        sed -i '/skyha/d' /etc/rc.d/rc.local
    fi
}

boot() {
    losetup $losetup_dev $data_dir/drbd.img
    systemctl start docker.service
    systemctl start pacemaker.service
    pcs cluster unstandby
}


solve_split_brain() {
    $WORK_DIR/resource/drbd 'brain'
}

show_status() {
    if [ ! -n "$master_ip" ] || [ ! -n "$slave_ip" ]; then
        echo "ERROR. master_ip or slave_ip is incorrect."
        return 0
    fi

    my_ip="$master_ip"
    hostname -I | grep "$slave_ip" >>/dev/null
    if [ $? -eq 0 ]; then
        my_ip="$slave_ip"
    fi

    if [ "$my_ip" == "$master_ip" ]; then
        master_data_dir_info=`du -sh $data_dir/*`
        master_drbd_dir_info=`du -sh /drbd/*`

        res=`ping -c 1 "$slave_ip"`
        if [ "$?" -ne 0 ]; then
            echo "ERROR. Ping $slave_ip failed."
        else
            slave_data_dir_info=`ssh root@$slave_ip "du -sh $data_dir/*"`
            slave_drbd_dir_info=`ssh root@$slave_ip "du -sh /drbd/*"`
        fi
    fi

    if [ "$my_ip" == "$slave_ip" ]; then
        res=`ping -c 1 "$master_ip"`
        if [ "$?" -ne 0 ]; then
            echo "ERROR. Ping $master_ip failed."
        else
            master_data_dir_info=`ssh root@$master_ip "du -sh $data_dir/*"`
            master_drbd_dir_info=`ssh root@$master_ip "du -sh /drbd/*"`
        fi

        slave_data_dir_info=`du -sh $data_dir/*`
        slave_drbd_dir_info=`du -sh /drbd/*`
    fi

    echo "
Base Info:
  * vip: $vip_master
  * $master_hostname: $master_ip
  * $slave_hostname: $slave_ip

Space Info:
* Node $master_hostname:
$master_data_dir_info
$master_drbd_dir_info
* Node $slave_hostname:
$slave_data_dir_info
$slave_drbd_dir_info

NFS Info:
`showmount -e $vip_master`
"
    crm_mon -Afr -1
    if [ $? -ne 0 ]; then
        echo "ERROR. Pacemaker may not start correctly."
    fi
}

show_status_as_xml() {
    base_info=`get_base_info`
    space_info=`get_space_info`
    nfs_info=`get_nfs_info`
    crm_mon_result=`get_crm_mon_result`

    status_xml="
<?xml version=\"1.0\"?>
<HAStatus>
    <BaseInfo>
        $base_info
    </BaseInfo>
    <SpaceInfo>
        $space_info
    </SpaceInfo>
    <NFSInfo>
        $nfs_info
    </NFSInfo>
    <PacemakerInfo>
        $crm_mon_result
    </PacemakerInfo>
</HAStatus>
    "
    echo $status_xml
}

get_base_info() {
    echo "
<node name=\"vip\" ip=\"$vip_master\"/>
<node name=\"$master_hostname\" ip=\"$master_ip\"/>
<node name=\"$slave_hostname\" ip=\"$slave_ip\"/>
"
}

get_space_info() {
    show_status > /var/log/.temp_show_status
    sed -n '/Space Info/,/NFS Info/p' /var/log/.temp_show_status > /var/log/.temp_space_info
    sed -i '/Space/d' /var/log/.temp_space_info
    sed -i '/NFS/d' /var/log/.temp_space_info
    sed -n '1,/Node/p' /var/log/.temp_space_info > /var/log/.temp_space_info_master
    sed -i '/Node/d' /var/log/.temp_space_info_master
    sed -n "/Node $slave_hostname/,//p" /var/log/.temp_space_info > /var/log/.temp_space_info_slave
    sed -i '/Node/d' /var/log/.temp_space_info_slave

    master_space_info=`get_space_info_from_file /var/log/.temp_space_info_master`
    slave_space_info=`get_space_info_from_file /var/log/.temp_space_info_slave`
    
    rm -f /var/log/.temp_*

    echo "
    <node name=\"$master_hostname\">
        $master_space_info
    </node>

    <node name=\"$slave_hostname\">
        $slave_space_info
    </node>
    "
}

get_space_info_from_file() {
    f=$1
    str=""
    index=0
    for item in `cat $f`
    do
        let "index=$index%2"
        [ $index -eq 0 ] && used=$item
        [ $index -eq 1 ] && dir=$item
        [ $index -eq 1 ] && str="$str <space dir=\"$item\" used=\"$used\"/>"
        let "index++"
    done
    echo $str
}

get_nfs_info() {
    nfssharedir=`showmount -e $vip_master | tail -1`
    echo "
<nfs ip=\"$vip_master\" nfssharedir=\"$nfssharedir\"/>
"
}

get_crm_mon_result() {
    crm_mon -X -Afr -1 > /var/log/.tempfile
    if [ $? -ne 0 ]; then
        peer_ip=`get_peer_ip`
        ssh root@$peer_ip "crm_mon -X -Afr -1" > /var/log/.tempfile
        if [ $? -ne 0 ]; then
            echo "<base is_run=\"false\" />"
            rm -f /var/log/.tempfile
            return 1
        fi
    fi

    sed -i '1d' /var/log/.tempfile
    echo "<base is_run=\"true\"/>" >> /var/log/.tempfile
    cat /var/log/.tempfile
    rm -f /var/log/.tempfile
}

get_peer_ip() {
    peer_ip=$master_ip
    ip addr | grep $master_ip >>/dev/null
    if [ $? -eq 0 ]; then
        peer_ip=$slave_ip
    fi
    echo $peer_ip
}
switch() {
    ip addr | grep $vip_master
    if [ $? -ne 0 ]; then
        echo "ERROR. This host is not master. Please do switch on master."
        return 1
    fi

    echo "Check ha status, please wait ..."
    check_ha_status
    if [ $? -ne 0 ]; then
        echo "ERROR. HA status is incorrect. It can not switch."
        return 1
    fi

    NODENAME=$master_hostname
    crm_mon -Afr -1 | grep Masters | grep $NODENAME
    if [ $? -ne 0 ]; then
        NODENAME=$slave_hostname
    fi

    echo "pcs cluster standby $NODENAME ..."
    pcs cluster standby $NODENAME

    try=1
    while [ $try -lt 60 ]
    do
        crm_mon -Afr -1 | grep 'Stopped:' | grep $NODENAME >> /dev/null
        if [ $? -eq 0 ]; then
            break
        fi
        sleep 1
        let "try++"
    done

    echo "pcs cluster unstandby $NODENAME ..."
    pcs cluster unstandby $NODENAME

    check_ha_status
    if [ $? -ne 0 ]; then
        echo "ERROR. Switch Failed."
        return 1
    else
        echo "OK. Switch succeed."
        return 0
    fi
}

version() {
    echo "version:1.0.0"
}

[ -f $WORK_DIR/conf/ha.conf ] && source $WORK_DIR/conf/ha.conf

# What kind of command ？
case "$1" in
    failover)                failover;;
    args_parse)              args_parse $@;;
    prepare)                 prepare;;    
    reset)                   reset;;
    recover)                 recover $2;;
    boot)                    boot;;
    install)                 install_all $@;;
    config)                  config $@;;
    standalone)              failover STANDALONE;;
    enable)                  enable $2;;
    disable)                 disable $2;;
    brain)                   solve_split_brain;;
    show)                    show_status;;
    show_xml)                show_status_as_xml;;
    switch-master-slave)     switch;;
    setup)                   setup $@;;
    version|-v|--version)    version;;
    help|usage|-h|--help)    usage;;
    *)                       usage;;

esac
